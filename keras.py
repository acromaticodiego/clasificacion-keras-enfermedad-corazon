# -*- coding: utf-8 -*-
"""keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uALBl7ijqacBgtUcqxzxSNIqyS4IJM6H
"""

# 1. Instalación de Kaggle CLI
!pip install kaggle --quiet

# 2. Configurar credenciales de Kaggle automáticamente
import os, json, time

# Carpeta de configuración de Kaggle
dir_kaggle = os.path.expanduser("~/.kaggle")
os.makedirs(dir_kaggle, exist_ok=True)

# Tus credenciales de Kaggle
creds = {
    "username": "diegoossa",
    "key": "71656ce9d7283c022f7777b1dadf5f84"
}
# Guardar kaggle.json
path_json = os.path.join(dir_kaggle, "kaggle.json")
with open(path_json, "w") as f:
    json.dump(creds, f)
# Ajustar permisos
os.chmod(path_json, 0o600)
# Variables de entorno
os.environ['KAGGLE_USERNAME'] = creds['username']
os.environ['KAGGLE_KEY'] = creds['key']

# 3. Descargar y descomprimir dataset de Kaggle
!kaggle datasets download -d redwankarimsony/heart-disease-data -q
!unzip -o heart-disease-data.zip

# 4. Detectar y listar CSV
import pandas as pd
import numpy as np

df_files = [f for f in os.listdir('.') if f.endswith('.csv')]
if not df_files:
    raise FileNotFoundError("No se encontró ningún CSV tras descomprimir.")
csv_file = df_files[0]
print(f"Usando CSV: {csv_file}")

# 5. Importar librerías para procesamiento
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

# 6. Cargar y mostrar datos
df = pd.read_csv(csv_file)
print("Shape del dataset:", df.shape)
print(df.head())

# 7. Análisis y reparación de datos
# 7.1 Función para analizar nulos y ceros
def print_nulls_and_zeros(df):
    print("\nValores nulos por columna:")
    print(df.isnull().sum())
    print("\nValores iguales a cero por columna:")
    print((df == 0).sum())

print_nulls_and_zeros(df)

# 7.2 Imputar ceros en columnas numéricas donde representan faltantes
to_impute = ['trestbps', 'chol', 'thalach']

for col in to_impute:
    if col not in df.columns:
        print(f"⚠️ Columna '{col}' no existe en el DataFrame. Se omite.")
        continue
    # 1) calculo la mediana ignorando ceros
    med = df.loc[df[col] != 0, col].median()
    # 2) reemplazo cada 0 por la mediana (sin usar inplace ni NaN intermedio)
    df[col] = df[col].mask(df[col] == 0, med)

print("\nDespués de imputar ceros:")
print_nulls_and_zeros(df)

# 8. Columna objetivo (clasificación binaria)
target_candidates = [c for c in ['target', 'num'] if c in df.columns]
if not target_candidates:
    raise KeyError('No se encontró columna objetivo.')
target_col = target_candidates[0]
print(f"Usando '{target_col}' como columna objetivo.")

# 9. Separar X e y
X = df.drop(target_col, axis=1)
y = df[target_col]

# 9.1 Eliminar columnas irrelevantes
X = X.drop(columns=['id','dataset'], errors='ignore')

# 9.2 Convertir y a binario: 0 = sin enfermedad, 1 = con enfermedad
y = (y > 0).astype(int)

# 10. Columnas categóricas y numéricas
categorical = ['sex','cp','fbs','restecg','exang','slope','ca','thal']
numerical   = [c for c in X.columns if c not in categorical]

# 11. Preprocesamiento
t = ColumnTransformer([
    ('num', StandardScaler(), numerical),
    ('cat', OneHotEncoder(sparse_output=False), categorical)
])
X_proc = t.fit_transform(X)

# 11.1 Eliminar filas con NaN después del preprocesamiento
X_df = pd.DataFrame(X_proc)
X_df['target'] = y.values
X_df_clean = X_df.dropna()

X_proc = X_df_clean.drop('target', axis=1).values
y = X_df_clean['target'].values
input_dim = X_proc.shape[1]
print(f"\nDimensión de entrada tras preprocesamiento limpio: {input_dim}")

# 12. División train/val/test (60/20/20)
X_tr, X_tmp, y_tr, y_tmp = train_test_split(
    X_proc, y,
    test_size=0.4,
    random_state=42,
    stratify=y
)
X_val, X_te, y_val, y_te = train_test_split(
    X_tmp, y_tmp,
    test_size=0.5,
    random_state=42,
    stratify=y_tmp
)
print(f"Tamaños → train: {len(y_tr)}, val: {len(y_val)}, test: {len(y_te)}")

# 13. Constructor de modelo
def build_model(arch, activation):
    model = keras.Sequential([keras.layers.Input(shape=(input_dim,))])
    for units in arch:
        model.add(keras.layers.Dense(units, activation=activation))
    model.add(keras.layers.Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

# 14. Topologías y activaciones a probar
topologies  = [[32], [64,32], [128,64,32]]
activations = ['sigmoid','tanh','relu']
results     = []

# 15. Entrenamiento y evaluación
tf.random.set_seed(42)
for act in activations:
    for topo in topologies:
        tf.keras.backend.clear_session()
        model = build_model(topo, act)
        start = time.time()
        hist  = model.fit(
            X_tr, y_tr,
            validation_data=(X_val,y_val),
            epochs=50,
            batch_size=32,
            verbose=0
        )
        elapsed = time.time() - start
        best_val_acc = max(hist.history['val_accuracy'])
        test_loss, test_acc = model.evaluate(X_te, y_te, verbose=0)
        results.append({
            'activation': act,
            'topology'  : topo,
            'val_acc'   : best_val_acc,
            'test_acc'  : test_acc,
            'test_loss' : test_loss,
            'train_time': elapsed,
            'hist'      : hist
        })
        print(f"Activación={act}, Topología={topo} → "
              f"val_acc={best_val_acc:.4f}, test_acc={test_acc:.4f}, "
              f"time={elapsed:.1f}s")

# 16. Mejor modelo por activación
print("\n---- Mejor modelo por cada activación ----")
best_per_activation = {}
for act in activations:
    subset = [r for r in results if r['activation']==act]
    best   = max(subset, key=lambda x: x['val_acc'])
    best_per_activation[act] = best
    print(f"{act.upper()}: Topología={best['topology']}, "
          f"val_acc={best['val_acc']:.4f}, "
          f"test_acc={best['test_acc']:.4f}, "
          f"test_loss={best['test_loss']:.4f}, "
          f"train_time={best['train_time']:.1f}s")

# 16. Mejor modelo por activación
print("\n---- Mejor modelo por cada activación ----")
best_per_activation = {}
for act in activations:
    subset = [r for r in results if r['activation'] == act]
    best = max(subset, key=lambda x: x['val_acc'])
    best_per_activation[act] = best
    print(f"{act.upper()}: Topología={best['topology']}, "
          f"val_acc={best['val_acc']:.4f}, "
          f"test_acc={best['test_acc']:.4f}, "
          f"test_loss={best['test_loss']:.4f}, "
          f"train_time={best['train_time']:.1f}s")

# 17. Obtener el mejor modelo basado en la activación
best_activation = 'sigmoid'  # Cambia esto a la activación que desees usar
best_model_info = best_per_activation[best_activation]

# Construir el modelo con la mejor topología
best_model = build_model(best_model_info['topology'], best_activation)

# Entrenar el modelo con los mejores parámetros (si no tienes los pesos guardados)
best_model.fit(X_tr, y_tr, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=0)

# 18. Realizar las predicciones con el modelo entrenado
y_pred = (best_model.predict(X_te) > 0.5).astype(int)

# 19. Imprimir la matriz de confusión y el reporte de clasificación
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_te, y_pred))
print(classification_report(y_te, y_pred))

# 17. Gráficas de entrenamiento
for act, r in best_per_activation.items():
    h = r['hist']
    plt.figure()
    plt.plot(h.history['accuracy'],    label='train_acc')
    plt.plot(h.history['val_accuracy'],label='val_acc')
    plt.title(f'Accuracy vs Épocas ({act})')
    plt.legend()
    plt.show()

    plt.figure()
    plt.plot(h.history['loss'],    label='train_loss')
    plt.plot(h.history['val_loss'],label='val_loss')
    plt.title(f'Loss vs Épocas ({act})')
    plt.legend()
    plt.show()